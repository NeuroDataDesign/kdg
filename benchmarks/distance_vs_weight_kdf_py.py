# -*- coding: utf-8 -*-
"""distance_vs_weight_kdf.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1za41K8WsLp7qcg4yi2MWqatw5ytq13d2
"""

#install libraries

!rm -r kdg
!git clone -b weighted_kdn https://github.com/NeuroDataDesign/kdg
!ls
!pip install kdg/.

# import modules
import numpy as np
from numpy.random import default_rng
from sklearn.ensemble import RandomForestClassifier as rf 
from kdg.kdf import *
from kdg.utils import gaussian_sparse_parity, trunk_sim
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# define the experimental setup
p = 20 # total dimensions of the data vector
p_star = 3 # number of signal dimensions of the data vector
'''sample_size = np.logspace(
        np.log10(10),
        np.log10(5000),
        num=10,
        endpoint=True,
        dtype=int
        )'''
sample_size = 5000 # sample size under consideration
n_test = 500 # test set size
nTrees = 100
compile_kwargs = {
    "n_estimators": nTrees
    }

def get_kdf_weights(X_star, X_, forest):
    polytope_star = np.array(
        [tree.apply(X_star.reshape(1, -1)) for tree in forest.estimators_]
        ).T
    #print(polytope_star.shape)
    predicted_leaf_ids_across_trees = np.array(
        [tree.apply(X_) for tree in forest.estimators_]
        ).T

    matched_samples = np.sum(
        predicted_leaf_ids_across_trees == polytope_star,
        axis=1
    )
    #print(matched_samples)
    idx = np.where(
        matched_samples>0
    )[0]

    #X_star isn't included in X_ so need to scale this manually
    scale = matched_samples[idx]/polytope_star.shape[1]
      
    return np.array(scale)

X, y = gaussian_sparse_parity(
    sample_size,
    p_star=p_star,
    p=p
)

X_test, y_test = gaussian_sparse_parity(
    n_test,
    p_star=p_star,
    p=p
)

#%%
# train Vanilla RF
vanilla_rf = rf(**compile_kwargs).fit(X, y)

# pick a random point from testing data 
rng = default_rng(0)
idx = rng.integers(0, sample_size)
X_star = X[idx]

#generate a bunch of points
distances = np.logspace(-40, 10, num=51, base=10)
X_dist = {}
for d in distances:
  #print(d)
  rand_dist = rng.random((n_test, p_star))
  rand_dist = np.sqrt(d*(rand_dist/np.sum(rand_dist, axis=1)[:,None]))
  noise = 2*rng.random((n_test,p-p_star))-1
  #noise = X_star[3:] + d*2*rng.random((n_test,p-p_star))-1
  X_dist[d] = np.concatenate((X_star[:3] + rand_dist, noise), axis=1)

# plot distance vs. weights
weights = [get_kdf_weights(X_star, X_dist[x], vanilla_rf) for x in X_dist]
weight_med = [np.median(w) for w in weights]
weight_25q = [np.quantile(w, 0.25) for w in weights]
weight_75q = [np.quantile(w, 0.75) for w in weights]
sns.set_context('talk')

fig, ax = plt.subplots(1,1, figsize=(8,8))
ax.set_xscale("log")
ax.plot(distances, weight_med, c="k", label='KDF')
ax.fill_between(distances, weight_25q, weight_75q, facecolor='r', alpha=.3)

noise

print(f"Mean number of samples per polytopes: {np.mean(RF_weight_sizes)} (range: {np.min(RF_weight_sizes)}, {np.max(RF_weight_sizes)})")
print(f"Mean weight of samples in polytopes: {np.mean(RF_weight_mean):.4f}")
print(f"Median weight of samples in polytopes: {np.median(RF_weight_median)}")