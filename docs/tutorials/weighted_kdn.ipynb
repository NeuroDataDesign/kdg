{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7m4Obg3xflW"
   },
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3z9g8zUAwdRM"
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from kdg.kdn import *\n",
    "from kdg.utils import gaussian_sparse_parity, trunk_sim\n",
    "from functions.weighted_kdn_tutorialfunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIlC_4YiyH0m"
   },
   "source": [
    "## Prepare dataset (Gaussian Sparse Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgE3DM1YyFxz"
   },
   "outputs": [],
   "source": [
    "# Define the experimental setup.\n",
    "# We will be using a Gaussian sparse parity setup\n",
    "# You can change these parameters.\n",
    "# Higher than 10,000 training samples will take a prohibitively long time to run\n",
    "\n",
    "p = 20  # total dimensions of the data vector\n",
    "p_star = 3  # number of signal dimensions of the data vector\n",
    "\n",
    "n_train = 2000\n",
    "n_test = 1000\n",
    "\n",
    "X, y = gaussian_sparse_parity(n_train, p_star=p_star, p=p)\n",
    "X_test, y_test = gaussian_sparse_parity(n_test, p_star=p_star, p=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COUhbYhCGk3m"
   },
   "source": [
    "## Raw Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8NLc6o8Gif-"
   },
   "outputs": [],
   "source": [
    "# Define our neural network parameters\n",
    "dense_layer_nodes = 5\n",
    "\n",
    "# NN params\n",
    "compile_kwargs = {\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"optimizer\": keras.optimizers.Adam(3e-4),\n",
    "}\n",
    "fit_kwargs = {\"epochs\": 150, \"batch_size\": 32, \"verbose\": False}\n",
    "\n",
    "\n",
    "# Fit the neural network using our test data and print accuracy\n",
    "raw_nn = getNN(dense_layer_nodes, p, **compile_kwargs)\n",
    "raw_nn.fit(X, keras.utils.to_categorical(y), **fit_kwargs)\n",
    "y_hat = np.argmax(raw_nn.predict(X_test), axis=1)\n",
    "\n",
    "print(f\"Raw NN Accuracy: {np.mean(y_hat == y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pb7FjE4Gnbe"
   },
   "source": [
    "## Kernel Density Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WkGkOhnYAe-"
   },
   "source": [
    "The goal of the kernel density network algorithm (KDN) is to reduce the overhead of a traditional neural network by using the neural network to fit Gaussian distributions to the polytope space. We will define polytopes by the nodes activated when data is passed through the neural network. Then subsequent data is classified according to the class of the polytope it most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11xrO-wl7K5y"
   },
   "outputs": [],
   "source": [
    "# Initialize KDN using previous neural network\n",
    "unweighted_kdn = kdn(network=raw_nn, weighted=False, verbose=True)\n",
    "unweighted_kdn.fit(X, y)\n",
    "\n",
    "y_hat = unweighted_kdn.predict(X_test)\n",
    "\n",
    "print(f\"Unweighted KDN Accuracy: {np.mean(y_hat == y_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7S_1auJxpPH"
   },
   "source": [
    "## Weighted Kernel Density Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKalNf7HGpE9"
   },
   "source": [
    "\n",
    "Neural networks are stochastic: two identical data samples should be processed by a neural network in an identical manner, activating the same nodes in each layer. This means one possible metric of similarity between two samples would be to compare the neural network activation pattern at each layer.\n",
    "\n",
    "Given that if a layer is significantly different, that difference will propagate through all subsequent layers, we iterate through the neural network one layer at a time, drawing nodes one at a time until a mismatched node is found.\n",
    "\n",
    "Let $N$ be the number of nodes in the network, $n$ be the number of nodes in a single layer of the network, and $m$ be the number of matches in a given layer, $(n \\leq N, m \\leq n)$.\n",
    "\n",
    "The probability of drawing exactly $k$ matched nodes, followed by a single mismatched node is:\n",
    "\n",
    "\\begin{align}P(k) = \\frac{1}{k+1} \\frac{{m \\choose k}{n-m \\choose 1}}{{n \\choose k+1}}\\end{align}\n",
    "\n",
    "i.e. the probability of drawing exactly $k$ matches and 1 mismatch, in that specific order.\n",
    "\n",
    "Then for each layer, we can calculate the expected weight, $E(W_i)$, by multiplying each weight $\\frac{k}{n}$ by the probability that exactly $k$ nodes will be drawn and adding them together:\n",
    "\n",
    "\\begin{align}\n",
    "E(W_i) & = \\sum_{k=0}^{m} \\frac{k}{n}\\frac{1}{k+1} \\frac{{m \\choose k}{n-m \\choose 1}}{{n \\choose k+1}}\\\\\n",
    "& = \\sum_{k=0}^{m} \\frac{k}{n(k+1)} \\frac{{m \\choose k}(n-m)}{{n \\choose k+1}}\\\\\n",
    "&....\\\\\n",
    "& = \\frac{1}{n-m+1}\\frac{{n-1 \\choose n-m}}{{n \\choose n-m}}\\\\\n",
    "& = \\frac{1}{n-m+1}\\frac{(n-1)!m!}{n!(m-1)!}\\\\\n",
    "& = \\frac{m}{n(n-m+1)}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Test the function below to confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73ZaA4r2GkfE",
    "outputId": "e8bb1f90-91ce-4470-f1c0-b888a08407f2"
   },
   "outputs": [],
   "source": [
    "reps = 10\n",
    "N = np.random.randint(5, 100, reps)\n",
    "M = [np.random.randint(n) for n in N]\n",
    "\n",
    "test = np.vstack((N, M))\n",
    "\n",
    "for t in test.T:\n",
    "    print(\n",
    "        f\"(n={t[0]}, m={t[1]}), Hypergeometric Weight: {hypergeom_weight(t[0], t[1])}, Simplified Weight: {simple_weight(t[0], t[1])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJQD0_d21esa"
   },
   "source": [
    "However, for 2 unrelated binary strings, the expected number of mismatches, i.e. the Hamming distance goes towards $\\frac{n}{2}$. Note that looking at ${n \\choose k}$, this value is largest when $k = \\frac{n}{2}$. If we only set $E(W_i) = 0$ when a perfect mismatch occurs, then we would expect almost every layer of sufficient size to have a match, and every network to match somewhere along its activation path.\n",
    "\n",
    "For example, given a neural network of size $(5,5,2)$, the odds a random activation path has zero matches are $2^{-12} = 2.44 \\times 10^{-4}$. This scales prohibitively with the size of the network, and requires setting a floor at $m \\geq \\frac{n}{N}$. If $m$ is below this floor, set $E(W_i) = 0$.\n",
    "\n",
    "However, this means our minimum weight for a given layer is now:\n",
    "$$E(W_i)=\\frac{n}{2n(n-\\frac{n}{2}+1)}=\\frac{1}{2n-n+2}=\\frac{1}{n+2}$$\n",
    "\n",
    "To make use of the full weighting space, we want to scale our weights from 0 to 1, both within this layer (eliminating the floor) as well as within the entire network. The size of a single layer within the larger network is $\\frac{n}{N}$.\n",
    "\n",
    "Therefore, our final weighting formula is:\n",
    "\\begin{align}\n",
    "E(W_i) & = \\frac{n}{N}\\frac{2(m-\\frac{n}{2})}{n(n-m+1)} \\\\\n",
    "& = \\frac{2m-n}{N(n-m+1)}, \\forall \\frac{n}{2} < m \\leq n\n",
    "\\end{align}\n",
    "\n",
    "Since if $m = \\frac{n}{2}$, $E(W_i) = 0$, we can skip the calculation in this instance. Compare the improvement versus the unweighted algorithm and the change in polytope membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyfErnnptp85"
   },
   "outputs": [],
   "source": [
    "# Initialize KDN using previous neural network\n",
    "weighted_kdn = kdn(network=raw_nn, weighted=True, verbose=True)\n",
    "weighted_kdn.fit(X, y)\n",
    "\n",
    "y_hat = weighted_kdn.predict(X_test)\n",
    "\n",
    "print(f\"Weighted KDN Accuracy: {np.mean(y_hat == y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7oQujEFGRFi"
   },
   "source": [
    "## Mean Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzBi4vL2cU1l"
   },
   "source": [
    "To validate the efficacy of our algorithm, we will test it on a variety of sample sizes. As the runtime can be prohibitive, a smaller set of points will be used. The larger dataset is also included (runtime ~5 hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34cQT2n5E2AM"
   },
   "outputs": [],
   "source": [
    "accuracy_nn = []\n",
    "accuracy_kdn = []\n",
    "accuracy_weighted = []\n",
    "\n",
    "sample_size = [1000, 2000, 5000, 10000]\n",
    "reps = 5\n",
    "\n",
    "# Full dataset\n",
    "# sample_size = [1000, 2000, 5000, 10000, 20000, 30000, 50000]\n",
    "# reps = 10\n",
    "\n",
    "test_size = min(sample_size)\n",
    "\n",
    "# Run tests\n",
    "for s in sample_size:\n",
    "    print(f\"Testing with {s} samples\")\n",
    "    for r in range(reps):\n",
    "        X, y = gaussian_sparse_parity(s, p_star=p_star, p=p)\n",
    "        X_test, y_test = gaussian_sparse_parity(test_size, p_star=p_star, p=p)\n",
    "\n",
    "        raw_nn = getNN(dense_layer_nodes, p)\n",
    "        raw_nn.fit(X, keras.utils.to_categorical(y), **fit_kwargs)\n",
    "\n",
    "        accuracy_nn.append(np.mean(np.argmax(raw_nn.predict(X_test), axis=1) == y_test))\n",
    "\n",
    "        unweighted_kdn = kdn(network=raw_nn, weighted=False, verbose=False)\n",
    "        unweighted_kdn.fit(X, y)\n",
    "\n",
    "        accuracy_kdn.append(np.mean(unweighted_kdn.predict(X_test) == y_test))\n",
    "\n",
    "        weighted_kdn = kdn(network=raw_nn, weighted=True, verbose=False)\n",
    "        weighted_kdn.fit(X, y)\n",
    "\n",
    "        accuracy_weighted.append(np.mean(weighted_kdn.predict(X_test) == y_test))\n",
    "    print(\"NN Accuracy:\", accuracy_nn[-reps:])\n",
    "    print(\"KDN-Unweighted Accuracy:\", accuracy_kdn[-reps:])\n",
    "    print(\"KDN-Weighted Accuracy:\", accuracy_weighted[-reps:])\n",
    "\n",
    "# Plot results\n",
    "shape = (len(sample_size), reps)\n",
    "err_nn = 1 - np.array(accuracy_nn).reshape(shape)\n",
    "err_kdn = 1 - np.array(accuracy_kdn).reshape(shape)\n",
    "err_weighted = 1 - np.array(accuracy_weighted).reshape(shape)\n",
    "\n",
    "error_rates = (err_weighted, err_nn, err_kdn)\n",
    "labels = (\"KDN-Weighted\", \"NN\", \"KDN-Unweighted\")\n",
    "\n",
    "plot_accuracy(sample_size, error_rates, labels)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "weighted_kdn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
